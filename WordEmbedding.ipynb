{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUwFfnedSuES"
      },
      "outputs": [],
      "source": [
        "# One hot encoding i.e converting the categories into numerical labels."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Steps to follow:\n",
        "\n",
        "# Convert Text to lower case\n",
        "# Tokenize the text\n",
        "# Get unique words\n",
        "# Get the integer/position of the words\n",
        "# create a vector of each word by marking its position as 1 and rest as 0\n",
        "# create a matrix of the found vectors.\n",
        "\n",
        "\n",
        "##  Convert Using Sklearn"
      ],
      "metadata": {
        "id": "7jwgpE13UkSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Convert text to lower case\n",
        "text = \"Can I eat the Pizza\"\n",
        "text = text.lower()\n",
        "# Step 2: Tokenize the text\n",
        "tokens = text.split()\n",
        "print(\"Tokens:\", tokens)\n",
        "# Step 3: Get unique words\n",
        "unique_words = list(set(tokens))\n",
        "print(\"Unique Words:\", unique_words)\n",
        "# Step 4: Get the integer/position of the words\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(tokens)\n",
        "print(\"Word Index:\", integer_encoded)\n",
        "\n",
        "# Step 5: One-hot encoding using scikit-learn\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "encoded_matrix = one_hot_encoder.fit_transform(np.array(tokens).reshape(-1, 1))\n",
        "print(\"One-Hot Encoded Matrix:\\n\", encoded_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fw2BNdZVkr5",
        "outputId": "f6cd39e4-5886-45bc-e3e6-bd2d6a0ebad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['can', 'i', 'eat', 'the', 'pizza']\n",
            "Unique Words: ['can', 'eat', 'i', 'pizza', 'the']\n",
            "Word Index: [0 2 1 4 3]\n",
            "One-Hot Encoded Matrix:\n",
            " [[1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note :\n",
        "\n",
        "# np.array(tokens).reshape(-1, 1) will transform it into 2D array i.e:\n",
        "\n",
        "# array([['can'],\n",
        "#        ['i'],\n",
        "#        ['eat'],\n",
        "#        ['the'],\n",
        "#        ['pizza']], dtype='<U5')"
      ],
      "metadata": {
        "id": "PftYRvz1fE-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag-of-Words with Python example"
      ],
      "metadata": {
        "id": "0qrzECp2gcFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Letâ€™s look at an easy example to understand the concepts previously explained. We could be interested in analyzing the reviews about Game of Thrones:\n",
        "\n",
        "# Review 1: Game of Thrones is an amazing tv series!\n",
        "\n",
        "# Review 2: Game of Thrones is the best tv series!\n",
        "\n",
        "# Review 3: Game of Thrones is so great"
      ],
      "metadata": {
        "id": "0sue5Jh4gdwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Steps:\n",
        "\n",
        "# Step 1: Convert text to lowercase\n",
        "\n",
        "# step 2: CountVectorizer, for a matrix creation where each row represents a review\n",
        "\n",
        "# Step 3: Tokenization (CountVectorizer handles tokenization)\n",
        "\n",
        "# Step 4: Get the feature names (unique words)\n",
        "\n",
        "# represents a review and each column represents a unique word in the corpus. The values in the matrix\n",
        "# represent the count of each word in the corresponding review."
      ],
      "metadata": {
        "id": "BgpCFwL0ht4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Step 1: Convert text to lowercase\n",
        "reviews = [\n",
        "    \"Game of Thrones is an amazing tv series!\",\n",
        "    \"Game of Thrones is the best tv series!\",\n",
        "    \"Game of Thrones is so great\"\n",
        "]\n",
        "reviews_lower = [review.lower() for review in reviews]\n",
        "\n",
        "# step 2: CountVectorizer, a matrix is created where each row represents a review\n",
        "#  and each column represents a unique word in the corpus\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Step 3: Tokenization (CountVectorizer handles tokenization)\n",
        "# Fit the vectorizer to the data and transform it\n",
        "X = vectorizer.fit_transform(reviews_lower)\n",
        "\n",
        "# Step 4: Get the feature names (unique words)\n",
        "# Now that the vectorizer is fitted, you can get the features\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(\"Feature Names (Unique Words):\", feature_names)\n",
        "\n",
        "\n",
        "print(\"\\nBag-of-Words Matrix:\")\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCuk8CG9ji_-",
        "outputId": "3bdabc02-243a-4cc7-d045-d2ea16ef99e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names (Unique Words): ['amazing' 'an' 'best' 'game' 'great' 'is' 'of' 'series' 'so' 'the'\n",
            " 'thrones' 'tv']\n",
            "\n",
            "Bag-of-Words Matrix:\n",
            "[[1 1 0 1 0 1 1 1 0 0 1 1]\n",
            " [0 0 1 1 0 1 1 1 0 1 1 1]\n",
            " [0 0 0 1 1 1 1 0 1 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Term Frequency Inverse Document Frequency (TFIDF)\n",
        "\n",
        "# TFIDF = TF * IDF\n",
        "\n",
        "# TF(term) = Number of times the term appears in document / total number of terms in the document\n",
        "\n",
        "# IDF(term) = log(total number of documents / Number of documents with term in it)"
      ],
      "metadata": {
        "id": "tjnIGhuU5i7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Steps:\n",
        "# Convert Text to Lower Case: (Handled by TfidfVectorizer)\n",
        "# Remove Stop Words: Using the built-in English stop words list from scikit-learn.\n",
        "# Tokenize and Vectorize: Using TfidfVectorizer to create TF-IDF vectors.\n",
        "# Output Feature Names and TF-IDF Matrix: Print the feature names and the resulting TF-IDF matrix."
      ],
      "metadata": {
        "id": "5sTt9kPb7d-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "sentence1 = \"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\"\n",
        "sentence2 = \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\n",
        "\n",
        "data = [sentence1, sentence2]\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = tfidf_vectorizer.fit_transform(data)\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b5wnzIm6UvC",
        "outputId": "33c0c98d-91ed-48d4-dc2a-174f34737dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['08452810075over18' '2005' '21st' '87121' 'amore' 'apply' 'available'\n",
            " 'buffet' 'bugis' 'cine' 'comp' 'crazy' 'cup' 'entry' 'fa' 'final' 'free'\n",
            " 'got' 'great' 'jurong' 'la' 'point' 'question' 'rate' 'receive' 'std'\n",
            " 'text' 'tkts' 'txt' 'wat' 'win' 'wkly' 'world']\n",
            "[[0.         0.         0.         0.         0.2773501  0.\n",
            "  0.2773501  0.2773501  0.2773501  0.2773501  0.         0.2773501\n",
            "  0.         0.         0.         0.         0.         0.2773501\n",
            "  0.2773501  0.2773501  0.2773501  0.2773501  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.2773501\n",
            "  0.         0.         0.2773501 ]\n",
            " [0.19611614 0.19611614 0.19611614 0.19611614 0.         0.19611614\n",
            "  0.         0.         0.         0.         0.19611614 0.\n",
            "  0.19611614 0.39223227 0.39223227 0.19611614 0.19611614 0.\n",
            "  0.         0.         0.         0.         0.19611614 0.19611614\n",
            "  0.19611614 0.19611614 0.19611614 0.19611614 0.19611614 0.\n",
            "  0.19611614 0.19611614 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.3193851 for 'available': The word \"available\" appears in sentence1 and has a certain importance, as indicated by its TF-IDF score.\n",
        "# 0.0 for '2005': The word \"2005\" does not appear in sentence1, hence its TF-IDF score is 0."
      ],
      "metadata": {
        "id": "PmmrDR7T8FQx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}