{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Te-zOryD-Ak"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. convert text to lower case:\n",
        "# It is necessary to convert the text to lower case as it is case sensitive.\n",
        "\n"
      ],
      "metadata": {
        "id": "SbhwFn6sEeFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n",
        "lower_text = text.lower()\n",
        "print(lower_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnZYdTKkEwFA",
        "outputId": "30ac2273-ef97-4244-9051-722ccc3316ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a demo text for nlp using nltk. full form of nltk is natural language toolkit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. word tokenize\n",
        "# Tokenize sentences to get the tokens of the text i.e breaking the sentences into words."
      ],
      "metadata": {
        "id": "rGIxOdHjE_d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt') # Download the 'punkt' resource"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGXQ2uXEFeqc",
        "outputId": "92cb10aa-fec8-442c-f6a3-cb5a930f549c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens = nltk.word_tokenize(text)\n",
        "print (word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tIXb_YZFCPL",
        "outputId": "5d3e96a0-d5e4-4d70-9c6f-26501b717403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'Demo', 'Text', 'for', 'NLP', 'using', 'NLTK', '.', 'Full', 'form', 'of', 'NLTK', 'is', 'Natural', 'Language', 'Toolkit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sent tokenize\n",
        "# Tokenize sentences if the there are more than 1 sentence i.e breaking the sentences to list of sentence."
      ],
      "metadata": {
        "id": "AzvWMJQ6FvEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_token = nltk.sent_tokenize(text)\n",
        "print (sent_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zU3dFktF5RK",
        "outputId": "4dce62cf-4900-4f03-9364-09fa1873231a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is a Demo Text for NLP using NLTK.', 'Full form of NLTK is Natural Language Toolkit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stop words removal\n",
        "# Remove irrelevant words using nltk stop words like is,the,a etc from the sentences as they donâ€™t carry any information."
      ],
      "metadata": {
        "id": "QwDRfGGuGAv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCw8ll3tGneZ",
        "outputId": "402fa014-9f0a-495e-d3f7-e3b85d6fbe6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopword = stopwords.words(\"english\")"
      ],
      "metadata": {
        "id": "qbu2z49wGPed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "removing_stopwords = [word for word in word_tokens if word not in stopword]\n",
        "print (removing_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTKydTM3GyJR",
        "outputId": "e03ab407-bf79-44d3-c3af-233da2781af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'Demo', 'Text', 'NLP', 'using', 'NLTK', '.', 'Full', 'form', 'NLTK', 'Natural', 'Language', 'Toolkit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  lemma\n",
        "# lemmatize the text so as to get its root form eg: functions,funtionality as function\n",
        "\n",
        "# Lemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a\n",
        "#  word down to its root meaning to identify similarities. For example,\n",
        "# a lemmatization algorithm would reduce the word better to its root word, or lemme, good."
      ],
      "metadata": {
        "id": "OWLl2vP3IEXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer #is based on The Porter Stemming Algorithm"
      ],
      "metadata": {
        "id": "HawI5JxfIRZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pXRpI1VI3a3",
        "outputId": "806c9bc9-5ca1-4272-96ed-8e3fca713819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
        "print (lemmatized_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8Ehl9rDIZga",
        "outputId": "8b8503b4-2235-453f-8f0f-a04e271b5da8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'Demo', 'Text', 'for', 'NLP', 'using', 'NLTK', '.', 'Full', 'form', 'of', 'NLTK', 'is', 'Natural', 'Language', 'Toolkit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "# stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form"
      ],
      "metadata": {
        "id": "qlw-Y-qvJPvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "m41LjOFlJRBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowball_stemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "Q8uKs3vxJXJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]\n",
        "print (stemmed_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLahCqDHJlFY",
        "outputId": "3404d526-99bc-4a5b-f0c8-26598850b69c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'a', 'demo', 'text', 'for', 'nlp', 'use', 'nltk', '.', 'full', 'form', 'of', 'nltk', 'is', 'natur', 'languag', 'toolkit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get word frequency\n",
        "# counting the word occurrence using FreqDist library"
      ],
      "metadata": {
        "id": "bpW3799XJrp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist"
      ],
      "metadata": {
        "id": "vBrbVqn1J0Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq = FreqDist(word_tokens)\n",
        "print (freq.most_common(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ACGvQrtJ4GA",
        "outputId": "8c6ce02b-ea01-44e9-bad5-0453247dd181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('is', 2), ('NLTK', 2), ('This', 1), ('a', 1), ('Demo', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pos(Part of Speech)tags\n",
        "# POS tag helps us to know the tags of each word like whether a word is noun, adjective etc."
      ],
      "metadata": {
        "id": "ss8K5tEPKjOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMZeGedvK0e5",
        "outputId": "618efb20-4f27-47f6-88d7-026eff449ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tag = nltk.pos_tag(word_tokens)\n",
        "print (pos_tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR_Q5ePJKrCh",
        "outputId": "15e03366-6367-4dc7-d1d6-fa33bea6c8ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('Demo', 'NNP'), ('Text', 'NNP'), ('for', 'IN'), ('NLP', 'NNP'), ('using', 'VBG'), ('NLTK', 'NNP'), ('.', '.'), ('Full', 'NNP'), ('form', 'NN'), ('of', 'IN'), ('NLTK', 'NNP'), ('is', 'VBZ'), ('Natural', 'JJ'), ('Language', 'NNP'), ('Toolkit', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NER\n",
        "# NER(Named Entity Recognition) is the process of getting the entity names"
      ],
      "metadata": {
        "id": "9g_uYSsLMj_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UlVMWUSNqQ_",
        "outputId": "d0712abc-3ba4-40b8-c77d-57bdd0a2dc76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0dkqBSTN0G1",
        "outputId": "459023e1-45c4-4ba7-afa4-0fcc679730c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"who is Barrack Obama\"\n",
        "word = nltk.word_tokenize(text)\n",
        "pos_tag = nltk.pos_tag(word)\n",
        "chunk = nltk.ne_chunk(pos_tag)\n",
        "NE = [\" \".join(w for w, t in ele) for ele in chunk if isinstance(ele, nltk.Tree)]\n",
        "print (NE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g8MFe1mMlmq",
        "outputId": "12725ce9-961e-48f3-fcac-7c9fb2d300b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Barrack Obama']\n"
          ]
        }
      ]
    }
  ]
}